{
 "cells": [
  {
   "cell_type": "code",
   "id": "db42a3ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T20:01:14.939254Z",
     "start_time": "2025-03-07T20:01:14.929252Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "03576c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T20:01:14.992252Z",
     "start_time": "2025-03-07T20:01:14.961252Z"
    }
   },
   "source": [
    "# Layer base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient):\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5ef1a959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T20:01:15.073251Z",
     "start_time": "2025-03-07T20:01:15.025252Z"
    }
   },
   "source": [
    "# Dense layer\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, output_gradient):\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        self.weights -= self.learning_rate * weights_gradient\n",
    "        self.biases -= self.learning_rate * np.sum(output_gradient, axis=0, keepdims=True)\n",
    "        return input_gradient\n",
    "\n",
    "# Activation layer with Relu \n",
    "class ReLu(Layer):\n",
    "    def __init__(self, activation, activation_derivative):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(input)\n",
    "        \n",
    "    def backward(self, output_gradient):\n",
    "        return np.multiply(output_gradient, self.activation_derivative(self.input))\n",
    "            \n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(x):\n",
    "        return np.where(x > 0, 1, 0)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "896cbbc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T20:01:15.148250Z",
     "start_time": "2025-03-07T20:01:15.117254Z"
    }
   },
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers=None):\n",
    "        self.layers = layers if layers is not None else []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_gradient):\n",
    "        for layer in reversed(self.layers):\n",
    "            output_gradient = layer.backward(output_gradient)\n",
    "        return output_gradient\n",
    "    \n",
    "    def train(self, x_train, y_train, epochs, batch_size=None, verbose=True):\n",
    "        # Use full batch if batch_size not specified\n",
    "        batch_size = x_train.shape[0] if batch_size is None else batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data at each epoch\n",
    "            indices = np.random.permutation(x_train.shape[0])\n",
    "            x_shuffled, y_shuffled = x_train[indices], y_train[indices]\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                # Get current batch\n",
    "                x_batch = x_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass and compute error\n",
    "                predictions = self.forward(x_batch)\n",
    "                error = predictions - y_batch\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(error)\n",
    "       \n",
    "            # Print progress\n",
    "            if verbose and epoch % 1 == 0:\n",
    "                loss = np.mean(np.square(self.forward(x_train) - y_train))\n",
    "                print(f\"Epoch {epoch}: loss={loss:.6f}\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "d562991d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T20:01:15.752518Z",
     "start_time": "2025-03-07T20:01:15.272503Z"
    }
   },
   "source": [
    "# Tests\n",
    "def test_initialization():\n",
    "    nn = NeuralNetwork()\n",
    "    assert nn.layers == [], \"Initialization failed: layers should be an empty list\"\n",
    "\n",
    "    # Softmax activation layer\n",
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        exp_values = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient):\n",
    "        # Initialize gradient array\n",
    "        input_gradient = np.zeros_like(output_gradient)\n",
    "\n",
    "        for i, (single_output, single_output_gradient) in enumerate(zip(self.output, output_gradient)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the softmax function\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate the gradient\n",
    "            input_gradient[i] = np.dot(jacobian_matrix, single_output_gradient)\n",
    "\n",
    "        return input_gradient\n",
    "    \n",
    "def test_add_layer():\n",
    "    nn = NeuralNetwork()\n",
    "    layer = Dense(3, 2)\n",
    "    nn.add(layer)\n",
    "    \n",
    "    assert nn.layers == [layer], \"Adding layer failed: layer not added correctly\"\n",
    "    \n",
    "def test_forward_pass():\n",
    "    nn = NeuralNetwork()\n",
    "    layer = Dense(3, 2)\n",
    "    nn.add(layer)\n",
    "    input_data = np.array([[1, 2, 3]])\n",
    "    output_data = nn.forward(input_data)\n",
    "    \n",
    "    assert output_data.shape == (1, 2), \"Forward pass failed: output shape not as expected\"\n",
    "    \n",
    "def test_backward_pass():\n",
    "    nn = NeuralNetwork()\n",
    "    layer = Dense(3, 2)\n",
    "    nn.add(layer)\n",
    "    input_data = np.array([[1, 2, 3]])\n",
    "    nn.forward(input_data)\n",
    "    output_gradient = np.array([[1, 2]])\n",
    "    input_gradient = nn.backward(output_gradient)\n",
    "    \n",
    "    assert input_gradient.shape == (1, 3), \"Backward pass failed: gradient shape not as expected\"\n",
    "    \n",
    "def test_training():\n",
    "    nn = NeuralNetwork()\n",
    "    layer = Dense(3, 2)\n",
    "    activation = ReLu(ReLu.relu, ReLu.relu_derivative)\n",
    "    nn.add(layer)\n",
    "    nn.add(activation)\n",
    "    x_train = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    y_train = np.array([[0, 1], [1, 0]])\n",
    "    nn.train(x_train, y_train, epochs=10, batch_size=1, verbose=True)\n",
    "    loss = np.mean(np.square(nn.forward(x_train) - y_train))\n",
    "\n",
    "    assert loss < 1, \"Training failed: loss not reduced as expected\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test_initialization()\n",
    "    test_add_layer()\n",
    "    test_forward_pass()\n",
    "    test_backward_pass()\n",
    "    test_training()\n",
    "    print(\"All tests passed!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.266219\n",
      "Epoch 1: loss=0.237855\n",
      "Epoch 2: loss=0.248618\n",
      "Epoch 3: loss=0.250086\n",
      "Epoch 4: loss=0.246440\n",
      "Epoch 5: loss=0.241795\n",
      "Epoch 6: loss=0.215387\n",
      "Epoch 7: loss=0.211570\n",
      "Epoch 8: loss=0.220052\n",
      "Epoch 9: loss=0.222087\n",
      "All tests passed!\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
